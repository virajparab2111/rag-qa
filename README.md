# **ğŸ“„ RAG Policy QA System**

A **Retrieval-Augmented Generation (RAG)** based question-answering system for policy documents (PDFs & text files).  
The system supports **dense retrieval + reranking**, **source citations**, **answer confidence scoring**, and **JSON-constrained outputs**, powered by **ChromaDB** and **Phi-3 Mini (Ollama)**.

## **ğŸš€ Features**

- ğŸ“š Multi-document ingestion (PDF, TXT)
- ğŸ” Semantic search using Sentence Transformers
- ğŸ” Cross-encoder reranking for high-precision retrieval
- ğŸ§  LLM-based answering using **Phi-3 Mini**
- ğŸ“Œ Source-level citations per answer bullet
- ğŸ“Š Confidence score for each answer
- ğŸ“¦ Fully local (no external APIs required)

## **ğŸ› ï¸ Setup Instructions**

### **1ï¸âƒ£ Install Python dependencies**

````bash
pip install -r requirements.txt
````

### **2ï¸âƒ£ Install Ollama**

Download and install Ollama from:

ğŸ‘‰ <https://ollama.com/download>

Verify installation:

````bash
ollama --version
````

### **3ï¸âƒ£ Pull the Phi-3 Mini model**


````bash
ollama pull phi3:mini
````
This downloads the LLM used for answering questions.

### **4ï¸âƒ£ Run the RAG pipeline**

````bash
python rag.py
````

The system will:

- Load documents from pdf_data/ and data/
- Build a ChromaDB vector store (if not already present)
- Perform retrieval, reranking, and answer generation

## **ğŸ§± Project Architecture**

{content: 
rag-policy-qa/

â”‚

â”œâ”€â”€ src/

â”‚ â”œâ”€â”€ data_loader.py # Loads PDFs & text files with metadata

â”‚ â”œâ”€â”€ embedding.py # Chunking & embedding pipeline

â”‚ â”œâ”€â”€ vectorstore.py # ChromaDB + cross-encoder reranking

â”‚ â”œâ”€â”€ search.py # RAG orchestration (retrieve â†’ prompt â†’ LLM)

â”‚ â”œâ”€â”€ prompt.py # Strict JSON-based RAG prompt template

â”‚ â”œâ”€â”€ utils.py # Context building & evaluation helpers

â”‚

â”œâ”€â”€ pdf_data/ # Policy PDFs (tracked intentionally)

â”œâ”€â”€ data/ # Text-based policy documents

â”‚

â”œâ”€â”€ rag.py # Main entry point

â”œâ”€â”€ requirements.txt # Python dependencies

â”œâ”€â”€ README.md # Project documentation

â””â”€â”€ .gitignore
}

## **ğŸ” Retrieval & Answer Flow**

- **Document Loading**
  - PDFs and text files are loaded with source metadata
- **Chunking & Embedding**
  - Documents are split into overlapping chunks
  - Embeddings generated using all-MiniLM-L6-v2
- **Vector Storage**
  - Chunks stored in **ChromaDB**
- **Dense Retrieval**
  - Top-K chunks retrieved via vector similarity
- **Cross-Encoder Reranking**
  - Retrieved chunks reranked using ms-marco-MiniLM
- **LLM Answering**
  - Context injected into a **strict JSON prompt**
  - Phi-3 Mini generates:
    - Bullet-point answers
    - Source citations
    - Confidence score

## **ğŸ“¤ Output Format (Guaranteed)**

```json
{

"answer": \[

{

"point": "Refund is granted if the train is cancelled by the railways.",

"sources": \["CancellationRulesforIRCTCTrain.pdf"\]

}

\],

"confidence": 0.95

}
```

## **âš ï¸ Notes**

- Vector databases (chroma_store/, chroma_db/) are **not committed**
- All inference runs **locally**
- No internet or paid APIs required after setup

## **ğŸ“Œ Future Improvements**

- Web UI for document upload & querying
- Dockerized deployment
- Advanced evaluation metrics (precision/recall)
- Support for multi-language policies

## **ğŸ“„ License**

This project is for educational and research purposes.
